{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAWTMQo3YILb"
      },
      "outputs": [],
      "source": [
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "# term_width = int(term_width)\n",
        "\n",
        "try:\n",
        "    _, term_width = os.popen('stty size', 'r').read().split()\n",
        "    term_width = int(term_width)\n",
        "except ValueError:  # Catch the error if terminal size cannot be determined\n",
        "    term_width = 80  # Default width if actual size can't be determined\n",
        "\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    lr = 0.1\n",
        "    resume = False\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "dtWLhE-FYbpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Transformations for the validation dataset\n",
        "transform_validation = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZW4eKf2YcPf",
        "outputId": "951533a0-f379-4665-f84d-cb9ce72dad92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /kaggle/input/cifar-10-batches-py/data_batch_1\n",
        "# root_dir = '/kaggle/input/cifar-10-batches-py'\n",
        "root_dir = '/content'\n",
        "train_files = [f'data_batch_{i}' for i in range(1, 6)]\n",
        "validation_files = ['test_batch']"
      ],
      "metadata": {
        "id": "6NOrG_fJYfd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "EBReOySoYqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpickle function\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "metadata": {
        "id": "Wh0y39JwYsTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Custom(Dataset):\n",
        "    def __init__(self, data_paths, root_dir, transform=None):\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        for file_name in data_paths:\n",
        "            file_path = os.path.join(root_dir, file_name)\n",
        "            entry = unpickle(file_path)\n",
        "            self.data.append(entry[b'data'])\n",
        "            self.targets.extend(entry[b'labels'])\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32).transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "daOH_En1Ytp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CIFAR10Custom(train_files, root_dir=root_dir, transform=transform_train)"
      ],
      "metadata": {
        "id": "Q2m0CUWtYzt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = CIFAR10Custom(validation_files, root_dir=root_dir, transform=transform_validation)"
      ],
      "metadata": {
        "id": "CW0KJ2X6Y1Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "validationloader = torch.utils.data.DataLoader(\n",
        "    validation_dataset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "oeWFsDFMY2t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNeXt in PyTorch.\n",
        "\n",
        "See the paper \"Aggregated Residual Transformations for Deep Neural Networks\" for more details.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''Grouped convolution block.'''\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        group_width = cardinality * bottleneck_width\n",
        "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(group_width)\n",
        "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(group_width)\n",
        "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*group_width:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*group_width)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
        "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
        "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
        "        # self.layer4 = self._make_layer(num_blocks[3], 2)\n",
        "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
        "\n",
        "    def _make_layer(self, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
        "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
        "        # Increase bottleneck_width by 2 after each stage.\n",
        "        self.bottleneck_width *= 2\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNeXt29_2x64d():\n",
        "    # return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n",
        "    return ResNeXt(num_blocks=[2,2,2], cardinality=4, bottleneck_width=32)\n",
        "\n",
        "def ResNeXt29_4x64d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n",
        "\n",
        "def ResNeXt29_8x64d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n",
        "\n",
        "def ResNeXt29_32x4d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n",
        "\n",
        "def test_resnext():\n",
        "    net = ResNeXt29_2x64d()\n",
        "    x = torch.randn(1,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "# test_resnext()"
      ],
      "metadata": {
        "id": "fVYtb6SjYNY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "# net = EfficientNetB0()\n",
        "# net = RegNetX_200MF()\n",
        "# net = SimpleDLA()\n",
        "\n",
        "net = net.to(device)\n",
        "print('==> Completed Building model..')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKgTBLNOY7_g",
        "outputId": "944b9872-0a8f-40b1-8d02-32dfd53b84bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n",
            "==> Completed Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4xqb4PfZE3e",
        "outputId": "c917f482-d6bb-4a33-8950-47676c550188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfu5IVylZGzH",
        "outputId": "60663b72-653c-4eb6-afc8-3431223e6f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]             192\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3          [-1, 128, 32, 32]           8,192\n",
            "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
            "            Conv2d-5          [-1, 128, 32, 32]          36,864\n",
            "       BatchNorm2d-6          [-1, 128, 32, 32]             256\n",
            "            Conv2d-7          [-1, 256, 32, 32]          32,768\n",
            "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
            "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
            "            Block-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-13          [-1, 128, 32, 32]             256\n",
            "           Conv2d-14          [-1, 128, 32, 32]          36,864\n",
            "      BatchNorm2d-15          [-1, 128, 32, 32]             256\n",
            "           Conv2d-16          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
            "            Block-18          [-1, 256, 32, 32]               0\n",
            "           Conv2d-19          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-20          [-1, 256, 32, 32]             512\n",
            "           Conv2d-21          [-1, 256, 16, 16]         147,456\n",
            "      BatchNorm2d-22          [-1, 256, 16, 16]             512\n",
            "           Conv2d-23          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-24          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-25          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-26          [-1, 512, 16, 16]           1,024\n",
            "            Block-27          [-1, 512, 16, 16]               0\n",
            "           Conv2d-28          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-29          [-1, 256, 16, 16]             512\n",
            "           Conv2d-30          [-1, 256, 16, 16]         147,456\n",
            "      BatchNorm2d-31          [-1, 256, 16, 16]             512\n",
            "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
            "            Block-34          [-1, 512, 16, 16]               0\n",
            "           Conv2d-35          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-36          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-37            [-1, 512, 8, 8]         589,824\n",
            "      BatchNorm2d-38            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-39           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-40           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-41           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-42           [-1, 1024, 8, 8]           2,048\n",
            "            Block-43           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-44            [-1, 512, 8, 8]         524,288\n",
            "      BatchNorm2d-45            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-46            [-1, 512, 8, 8]         589,824\n",
            "      BatchNorm2d-47            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-48           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-49           [-1, 1024, 8, 8]           2,048\n",
            "            Block-50           [-1, 1024, 8, 8]               0\n",
            "           Linear-51                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 4,648,778\n",
            "Trainable params: 4,648,778\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 47.50\n",
            "Params size (MB): 17.73\n",
            "Estimated Total Size (MB): 65.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "w8E8J7wxZILr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Calculate average loss and accuracy over the epoch\n",
        "    train_loss_avg = train_loss / len(trainloader.dataset)\n",
        "    train_accuracy = 100. * correct / total\n",
        "    return train_loss_avg, train_accuracy"
      ],
      "metadata": {
        "id": "l6QjZ_4RbABR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(validationloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(validationloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "    # Calculate average loss and accuracy over the epoch\n",
        "    val_loss_avg = test_loss / len(validationloader.dataset)\n",
        "    val_accuracy = 100. * correct / total\n",
        "    return val_loss_avg, val_accuracy"
      ],
      "metadata": {
        "id": "ycRpyCqJbCRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "    train_loss, train_acc = train(epoch)\n",
        "    val_loss, val_acc = validate(epoch)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyZ9_yLpbDiJ",
        "outputId": "6f863a6e-e002-4878-d9a5-82d9e724f94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [================================================================>]  Step: 6s479ms | Tot: 1m46s | Loss: 2.144 | Acc: 25.752% (12876/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 72ms | Tot: 7s170ms | Loss: 1.675 | Acc: 36.070% (3607/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            " [================================================================>]  Step: 181ms | Tot: 1m42s | Loss: 1.549 | Acc: 42.406% (21203/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 75ms | Tot: 7s196ms | Loss: 1.601 | Acc: 44.070% (4407/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            " [================================================================>]  Step: 174ms | Tot: 1m43s | Loss: 1.244 | Acc: 54.864% (27432/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 77ms | Tot: 7s87ms | Loss: 1.231 | Acc: 56.570% (5657/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            " [================================================================>]  Step: 173ms | Tot: 1m43s | Loss: 1.007 | Acc: 64.028% (32014/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 67ms | Tot: 7s41ms | Loss: 1.111 | Acc: 60.260% (6026/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            " [================================================================>]  Step: 173ms | Tot: 1m43s | Loss: 0.878 | Acc: 68.808% (34404/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 76ms | Tot: 7s213ms | Loss: 1.215 | Acc: 59.810% (5981/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 5\n",
            " [================================================================>]  Step: 179ms | Tot: 1m43s | Loss: 0.804 | Acc: 71.478% (35739/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 76ms | Tot: 7s107ms | Loss: 0.974 | Acc: 66.150% (6615/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            " [================================================================>]  Step: 173ms | Tot: 1m43s | Loss: 0.745 | Acc: 73.798% (36899/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 67ms | Tot: 7s281ms | Loss: 0.957 | Acc: 67.820% (6782/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            " [================================================================>]  Step: 183ms | Tot: 1m43s | Loss: 0.698 | Acc: 75.450% (37725/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 69ms | Tot: 7s388ms | Loss: 1.143 | Acc: 65.170% (6517/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 8\n",
            " [================================================================>]  Step: 177ms | Tot: 1m44s | Loss: 0.666 | Acc: 76.762% (38381/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 67ms | Tot: 7s329ms | Loss: 0.812 | Acc: 72.040% (7204/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 9\n",
            " [================================================================>]  Step: 179ms | Tot: 1m43s | Loss: 0.636 | Acc: 77.718% (38859/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 74ms | Tot: 7s256ms | Loss: 0.852 | Acc: 72.040% (7204/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 10\n",
            " [================================================================>]  Step: 173ms | Tot: 1m43s | Loss: 0.601 | Acc: 79.118% (39559/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 72ms | Tot: 7s332ms | Loss: 0.950 | Acc: 70.240% (7024/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 11\n",
            " [================================================================>]  Step: 170ms | Tot: 1m44s | Loss: 0.563 | Acc: 80.400% (40200/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 84ms | Tot: 7s319ms | Loss: 0.702 | Acc: 75.980% (7598/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 12\n",
            " [================================================================>]  Step: 172ms | Tot: 1m43s | Loss: 0.545 | Acc: 81.144% (40572/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 72ms | Tot: 7s423ms | Loss: 0.776 | Acc: 73.580% (7358/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 13\n",
            " [================================================================>]  Step: 175ms | Tot: 1m44s | Loss: 0.524 | Acc: 81.964% (40982/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 63ms | Tot: 7s325ms | Loss: 0.714 | Acc: 76.830% (7683/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            " [================================================================>]  Step: 175ms | Tot: 1m43s | Loss: 0.509 | Acc: 82.242% (41121/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 76ms | Tot: 7s317ms | Loss: 0.709 | Acc: 75.230% (7523/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 15\n",
            " [================================================================>]  Step: 179ms | Tot: 1m43s | Loss: 0.495 | Acc: 82.798% (41399/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 81ms | Tot: 7s354ms | Loss: 0.741 | Acc: 76.840% (7684/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 16\n",
            " [================================================================>]  Step: 175ms | Tot: 1m43s | Loss: 0.480 | Acc: 83.380% (41690/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 72ms | Tot: 7s237ms | Loss: 0.596 | Acc: 80.060% (8006/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 17\n",
            " [================================================================>]  Step: 173ms | Tot: 1m44s | Loss: 0.471 | Acc: 83.762% (41881/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 80ms | Tot: 7s239ms | Loss: 0.839 | Acc: 73.090% (7309/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 18\n",
            " [================================================================>]  Step: 175ms | Tot: 1m43s | Loss: 0.460 | Acc: 84.098% (42049/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 68ms | Tot: 7s179ms | Loss: 0.648 | Acc: 77.690% (7769/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 19\n",
            " [================================================================>]  Step: 174ms | Tot: 1m44s | Loss: 0.448 | Acc: 84.638% (42319/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 70ms | Tot: 7s30ms | Loss: 0.653 | Acc: 79.010% (7901/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 20\n",
            " [================================================================>]  Step: 176ms | Tot: 1m43s | Loss: 0.451 | Acc: 84.614% (42307/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 73ms | Tot: 7s122ms | Loss: 0.688 | Acc: 77.550% (7755/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 21\n",
            " [================================================================>]  Step: 175ms | Tot: 1m43s | Loss: 0.440 | Acc: 84.714% (42357/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 69ms | Tot: 7s100ms | Loss: 0.598 | Acc: 79.670% (7967/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 22\n",
            " [================================================================>]  Step: 177ms | Tot: 1m43s | Loss: 0.429 | Acc: 85.236% (42618/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 68ms | Tot: 7s387ms | Loss: 0.706 | Acc: 76.580% (7658/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 23\n",
            " [================================================================>]  Step: 175ms | Tot: 1m44s | Loss: 0.424 | Acc: 85.310% (42655/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 74ms | Tot: 7s432ms | Loss: 0.573 | Acc: 80.600% (8060/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 24\n",
            " [================================================================>]  Step: 182ms | Tot: 1m43s | Loss: 0.419 | Acc: 85.554% (42777/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 64ms | Tot: 7s794ms | Loss: 0.548 | Acc: 82.200% (8220/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 25\n",
            " [================================================================>]  Step: 182ms | Tot: 1m44s | Loss: 0.414 | Acc: 85.756% (42878/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 68ms | Tot: 7s381ms | Loss: 0.707 | Acc: 77.010% (7701/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 26\n",
            " [================================================================>]  Step: 176ms | Tot: 1m43s | Loss: 0.405 | Acc: 86.078% (43039/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 68ms | Tot: 7s490ms | Loss: 0.538 | Acc: 82.090% (8209/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 27\n",
            " [================================================================>]  Step: 174ms | Tot: 1m44s | Loss: 0.402 | Acc: 86.128% (43064/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 79ms | Tot: 7s381ms | Loss: 0.800 | Acc: 75.980% (7598/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 28\n",
            " [================================================================>]  Step: 181ms | Tot: 1m44s | Loss: 0.395 | Acc: 86.416% (43208/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            " [================================================================>]  Step: 68ms | Tot: 7s421ms | Loss: 0.641 | Acc: 79.120% (7912/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "\n",
            "Epoch: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClLEnT12bE3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}