{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73233,"databundleVersionId":8112053,"sourceType":"competition"},{"sourceId":8077749,"sourceType":"datasetVersion","datasetId":4767291}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T02:10:52.368664Z","iopub.execute_input":"2024-04-12T02:10:52.369404Z","iopub.status.idle":"2024-04-12T02:10:53.168959Z","shell.execute_reply.started":"2024-04-12T02:10:52.369372Z","shell.execute_reply":"2024-04-12T02:10:53.168037Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar_test_nolabels.pkl\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_1\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_2\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/batches.meta\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/test_batch\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_3\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_5\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_4\n/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/readme.html\n/kaggle/input/checkpoint/ckpt.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport time\nimport math\nimport pickle\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import Dataset\nimport torchvision\nimport torchvision.transforms as transforms\n\n!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:15:19.875304Z","iopub.execute_input":"2024-04-12T02:15:19.876151Z","iopub.status.idle":"2024-04-12T02:15:33.110690Z","shell.execute_reply.started":"2024-04-12T02:15:19.876117Z","shell.execute_reply":"2024-04-12T02:15:33.109751Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# Unpickle function\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nclass CIFAR10Custom(Dataset):\n    def __init__(self, data_paths, root_dir, transform=None):\n        self.data = []\n        self.targets = []\n        for file_name in data_paths:\n            file_path = os.path.join(root_dir, file_name)\n            entry = unpickle(file_path)\n            self.data.append(entry[b'data'])\n            self.targets.extend(entry[b'labels'])\n        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32).transpose((0, 2, 3, 1))  # convert to HWC\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, index):\n        img, target = self.data[index], self.targets[index]\n        img = Image.fromarray(img)\n        if self.transform:\n            img = self.transform(img)\n        return img, target\n\n\nroot_dir = '/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/'\ntrain_files = [f'data_batch_{i}' for i in range(1, 6)]\nvalidation_files = ['test_batch']\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Transformations for the validation dataset\ntransform_validation = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Datasets for the train and validation subsets\ntrain_dataset = CIFAR10Custom(train_files, root_dir=root_dir, transform=transform_train)\nvalidation_dataset = CIFAR10Custom(validation_files, root_dir=root_dir, transform=transform_validation)\n\n# DataLoaders for the train and validation subsets\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\nvalidationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=100, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:16:43.743328Z","iopub.execute_input":"2024-04-12T02:16:43.743976Z","iopub.status.idle":"2024-04-12T02:16:46.471442Z","shell.execute_reply.started":"2024-04-12T02:16:43.743943Z","shell.execute_reply":"2024-04-12T02:16:46.470644Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model: Resnet 18 Modified to 4.6 million parameter","metadata":{}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 242, num_blocks[2], stride=2) \n        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n        \n        self.linear = nn.Linear(256*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet4m():\n    return ResNet(BasicBlock, [2, 2, 2, 2])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:21:07.528951Z","iopub.execute_input":"2024-04-12T02:21:07.529351Z","iopub.status.idle":"2024-04-12T02:21:07.727209Z","shell.execute_reply.started":"2024-04-12T02:21:07.529319Z","shell.execute_reply":"2024-04-12T02:21:07.726071Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"'''\nclass Args:\n    lr = 0.1\n    resume = False\n\nargs = Args()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-11T19:37:03.633177Z","iopub.execute_input":"2024-04-11T19:37:03.633451Z","iopub.status.idle":"2024-04-11T19:37:03.645492Z","shell.execute_reply.started":"2024-04-11T19:37:03.633428Z","shell.execute_reply":"2024-04-11T19:37:03.644590Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\nterm_width = 80  # Default width if actual size can't be determined\n\nTOTAL_BAR_LENGTH = 35.\nlast_time = time.time()\nbegin_time = last_time\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:58:10.085667Z","iopub.execute_input":"2024-04-12T02:58:10.086470Z","iopub.status.idle":"2024-04-12T02:58:10.106908Z","shell.execute_reply.started":"2024-04-12T02:58:10.086442Z","shell.execute_reply":"2024-04-12T02:58:10.105936Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n#           'dog', 'frog', 'horse', 'ship', 'truck')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T19:37:08.815951Z","iopub.execute_input":"2024-04-11T19:37:08.816218Z","iopub.status.idle":"2024-04-11T19:37:09.064132Z","shell.execute_reply.started":"2024-04-11T19:37:08.816194Z","shell.execute_reply":"2024-04-11T19:37:09.063001Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"==> Building model..\n==> Completed Building model..\n","output_type":"stream"}]},{"cell_type":"code","source":"net = ResNet4m()\nnet = net.to(device)\nsummary(net, input_size=(3, 32, 32))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:58:16.842076Z","iopub.execute_input":"2024-04-12T02:58:16.842935Z","iopub.status.idle":"2024-04-12T02:58:17.681380Z","shell.execute_reply.started":"2024-04-12T02:58:16.842900Z","shell.execute_reply":"2024-04-12T02:58:17.680404Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           1,728\n       BatchNorm2d-2           [-1, 64, 32, 32]             128\n            Conv2d-3           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-4           [-1, 64, 32, 32]             128\n            Conv2d-5           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-6           [-1, 64, 32, 32]             128\n        BasicBlock-7           [-1, 64, 32, 32]               0\n            Conv2d-8           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-9           [-1, 64, 32, 32]             128\n           Conv2d-10           [-1, 64, 32, 32]          36,864\n      BatchNorm2d-11           [-1, 64, 32, 32]             128\n       BasicBlock-12           [-1, 64, 32, 32]               0\n           Conv2d-13          [-1, 128, 16, 16]          73,728\n      BatchNorm2d-14          [-1, 128, 16, 16]             256\n           Conv2d-15          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-16          [-1, 128, 16, 16]             256\n           Conv2d-17          [-1, 128, 16, 16]           8,192\n      BatchNorm2d-18          [-1, 128, 16, 16]             256\n       BasicBlock-19          [-1, 128, 16, 16]               0\n           Conv2d-20          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-21          [-1, 128, 16, 16]             256\n           Conv2d-22          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-23          [-1, 128, 16, 16]             256\n       BasicBlock-24          [-1, 128, 16, 16]               0\n           Conv2d-25            [-1, 242, 8, 8]         278,784\n      BatchNorm2d-26            [-1, 242, 8, 8]             484\n           Conv2d-27            [-1, 242, 8, 8]         527,076\n      BatchNorm2d-28            [-1, 242, 8, 8]             484\n           Conv2d-29            [-1, 242, 8, 8]          30,976\n      BatchNorm2d-30            [-1, 242, 8, 8]             484\n       BasicBlock-31            [-1, 242, 8, 8]               0\n           Conv2d-32            [-1, 242, 8, 8]         527,076\n      BatchNorm2d-33            [-1, 242, 8, 8]             484\n           Conv2d-34            [-1, 242, 8, 8]         527,076\n      BatchNorm2d-35            [-1, 242, 8, 8]             484\n       BasicBlock-36            [-1, 242, 8, 8]               0\n           Conv2d-37            [-1, 256, 4, 4]         557,568\n      BatchNorm2d-38            [-1, 256, 4, 4]             512\n           Conv2d-39            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-40            [-1, 256, 4, 4]             512\n           Conv2d-41            [-1, 256, 4, 4]          61,952\n      BatchNorm2d-42            [-1, 256, 4, 4]             512\n       BasicBlock-43            [-1, 256, 4, 4]               0\n           Conv2d-44            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-45            [-1, 256, 4, 4]             512\n           Conv2d-46            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-47            [-1, 256, 4, 4]             512\n       BasicBlock-48            [-1, 256, 4, 4]               0\n           Linear-49                   [-1, 10]           2,570\n================================================================\nTotal params: 4,962,922\nTrainable params: 4,962,922\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 10.79\nParams size (MB): 18.93\nEstimated Total Size (MB): 29.74\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"if device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:58:52.651075Z","iopub.execute_input":"2024-04-12T02:58:52.651699Z","iopub.status.idle":"2024-04-12T02:58:52.659863Z","shell.execute_reply.started":"2024-04-12T02:58:52.651666Z","shell.execute_reply":"2024-04-12T02:58:52.659139Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n    \n    # Calculate average loss and accuracy over the epoch\n    train_loss_avg = train_loss / len(trainloader.dataset)\n    train_accuracy = 100. * correct / total\n    return train_loss_avg, train_accuracy\n\ndef validate(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(validationloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(validationloader), 'Val Loss: %.3f | Val Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/ckpt.pth')\n        best_acc = acc\n        \n    # Calculate average loss and accuracy over the epoch\n    val_loss_avg = test_loss / len(validationloader.dataset)\n    val_accuracy = 100. * correct / total\n    return val_loss_avg, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:58:57.226245Z","iopub.execute_input":"2024-04-12T02:58:57.226944Z","iopub.status.idle":"2024-04-12T02:58:57.240195Z","shell.execute_reply.started":"2024-04-12T02:58:57.226913Z","shell.execute_reply":"2024-04-12T02:58:57.239276Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"best_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\nnum_epochs = 200\n'''\ncheckpoint_file = '/kaggle/input/checkpoint/ckpt.pth'\ncheckpoint = torch.load(checkpoint_file)\n# Load the model's state_dict from the checkpoint\nnet.load_state_dict(checkpoint['net'])\nbest_acc = checkpoint['acc']\nstart_epoch = checkpoint['epoch']\n'''\n\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(start_epoch, start_epoch+num_epochs):\n    train_loss, train_acc = train(epoch)\n    val_loss, val_acc = validate(epoch)\n    \n    # Append metrics to lists\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:59:04.154816Z","iopub.execute_input":"2024-04-12T02:59:04.155632Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nEpoch: 0\n [==================================>]  Step: 1s471ms | Tot: 21s293ms | Train Loss: 1.693 | Train Acc: 37.222% (18611/50000 391/391 \n [==================================>]  Step: 13ms | Tot: 1s881ms | Val Loss: 1.433 | Val Acc: 48.030% (4803/10000 100/100 ============================>....]  Step: 25ms | Tot: 1s648ms | Val Loss: 1.435 | Val Acc: 48.000% (4176/8700 87/100 \nSaving..\n\nEpoch: 1\n [==================================>]  Step: 34ms | Tot: 19s860ms | Train Loss: 1.226 | Train Acc: 55.570% (27785/50000 391/391 ==============================>....]  Step: 50ms | Tot: 17s188ms | Train Loss: 1.248 | Train Acc: 54.764% (23693/43264 338/391 \n [==================================>]  Step: 13ms | Tot: 1s954ms | Val Loss: 1.242 | Val Acc: 56.440% (5644/10000 100/100 ================>...............]  Step: 19ms | Tot: 1s109ms | Val Loss: 1.232 | Val Acc: 57.328% (3325/5800 58/100 \nSaving..\n\nEpoch: 2\n [==================================>]  Step: 34ms | Tot: 19s868ms | Train Loss: 0.973 | Train Acc: 65.752% (32876/50000 391/391 =>.............................]  Step: 50ms | Tot: 3s90ms | Train Loss: 1.056 | Train Acc: 62.853% (4988/7936 62/391 \n [==================================>]  Step: 13ms | Tot: 1s959ms | Val Loss: 1.011 | Val Acc: 64.930% (6493/10000 100/100 \nSaving..\n\nEpoch: 3\n [==================================>]  Step: 34ms | Tot: 19s805ms | Train Loss: 0.797 | Train Acc: 72.206% (36103/50000 391/391 ==================>................]  Step: 50ms | Tot: 10s372ms | Train Loss: 0.813 | Train Acc: 71.421% (18741/26240 205/391 =======================>...........]  Step: 50ms | Tot: 13s110ms | Train Loss: 0.814 | Train Acc: 71.492% (23701/33152 259/391 \n [==================================>]  Step: 17ms | Tot: 1s786ms | Val Loss: 0.763 | Val Acc: 73.650% (7365/10000 100/100 ======>........................]  Step: 15ms | Tot: 559ms | Val Loss: 0.755 | Val Acc: 73.733% (2212/3000 30/100 ==================>................]  Step: 17ms | Tot: 969ms | Val Loss: 0.752 | Val Acc: 74.185% (4006/5400 54/100 ===========================>.......]  Step: 15ms | Tot: 1s391ms | Val Loss: 0.759 | Val Acc: 73.861% (5835/7900 79/100 \nSaving..\n\nEpoch: 4\n [==================================>]  Step: 34ms | Tot: 19s869ms | Train Loss: 0.684 | Train Acc: 76.132% (38066/50000 391/391 =====>..........................]  Step: 52ms | Tot: 4s766ms | Train Loss: 0.714 | Train Acc: 74.983% (9022/12032 94/391 \n [==================================>]  Step: 15ms | Tot: 1s867ms | Val Loss: 0.724 | Val Acc: 75.010% (7501/10000 100/100 \nSaving..\n\nEpoch: 5\n [==================================>]  Step: 34ms | Tot: 19s879ms | Train Loss: 0.612 | Train Acc: 78.898% (39449/50000 391/391 ========================>..........]  Step: 51ms | Tot: 13s799ms | Train Loss: 0.617 | Train Acc: 78.794% (27332/34688 271/391 ================================>..]  Step: 50ms | Tot: 18s628ms | Train Loss: 0.612 | Train Acc: 78.876% (36952/46848 366/391 \n [==================================>]  Step: 14ms | Tot: 1s830ms | Val Loss: 0.746 | Val Acc: 75.220% (7522/10000 100/100 ===>...........................]  Step: 16ms | Tot: 407ms | Val Loss: 0.756 | Val Acc: 74.957% (1724/2300 23/100 \nSaving..\n\nEpoch: 6\n [==================================>]  Step: 33ms | Tot: 19s799ms | Train Loss: 0.556 | Train Acc: 80.912% (40456/50000 391/391 ====>...........................]  Step: 50ms | Tot: 4s122ms | Train Loss: 0.559 | Train Acc: 80.736% (8474/10496 82/391 \n [==================================>]  Step: 14ms | Tot: 2s12ms | Val Loss: 0.580 | Val Acc: 79.900% (7990/10000 100/100 ...............................]  Step: 22ms | Tot: 226ms | Val Loss: 0.581 | Val Acc: 78.750% (945/1200 12/100 \nSaving..\n\nEpoch: 7\n [==================================>]  Step: 34ms | Tot: 19s773ms | Train Loss: 0.525 | Train Acc: 81.958% (40979/50000 391/391 >..............................]  Step: 50ms | Tot: 2s646ms | Train Loss: 0.520 | Train Acc: 82.120% (5571/6784 53/391 ============================>......]  Step: 50ms | Tot: 15s937ms | Train Loss: 0.526 | Train Acc: 81.848% (33001/40320 315/391 ==============================>....]  Step: 51ms | Tot: 17s153ms | Train Loss: 0.526 | Train Acc: 81.863% (35522/43392 339/391 ==============================>....]  Step: 50ms | Tot: 17s253ms | Train Loss: 0.526 | Train Acc: 81.885% (35741/43648 341/391 \n [==================================>]  Step: 13ms | Tot: 1s905ms | Val Loss: 0.861 | Val Acc: 73.860% (7386/10000 100/100 =======>.......................]  Step: 18ms | Tot: 625ms | Val Loss: 0.886 | Val Acc: 73.400% (2569/3500 35/100 \n\nEpoch: 8\n [==================================>]  Step: 33ms | Tot: 19s882ms | Train Loss: 0.506 | Train Acc: 82.650% (41325/50000 391/391 \n [==================================>]  Step: 15ms | Tot: 1s825ms | Val Loss: 0.621 | Val Acc: 79.350% (7935/10000 100/100 \n\nEpoch: 9\n [==================================>]  Step: 34ms | Tot: 19s887ms | Train Loss: 0.479 | Train Acc: 83.522% (41761/50000 391/391 ========>.........................]  Step: 50ms | Tot: 5s572ms | Train Loss: 0.478 | Train Acc: 83.395% (11742/14080 110/391 ============================>......]  Step: 51ms | Tot: 16s361ms | Train Loss: 0.480 | Train Acc: 83.504% (34417/41216 322/391 \n [==================================>]  Step: 13ms | Tot: 1s857ms | Val Loss: 0.600 | Val Acc: 79.810% (7981/10000 100/100 \n\nEpoch: 10\n [==================================>]  Step: 33ms | Tot: 19s835ms | Train Loss: 0.471 | Train Acc: 83.852% (41926/50000 391/391 .............................]  Step: 51ms | Tot: 877ms | Train Loss: 0.473 | Train Acc: 83.550% (1925/2304 18/391 =====================>.............]  Step: 51ms | Tot: 12s413ms | Train Loss: 0.467 | Train Acc: 83.957% (26329/31360 245/391 \n [==================================>]  Step: 14ms | Tot: 1s809ms | Val Loss: 0.616 | Val Acc: 79.650% (7965/10000 100/100 ==>............................]  Step: 17ms | Tot: 325ms | Val Loss: 0.610 | Val Acc: 79.650% (1593/2000 20/100 ======================>............]  Step: 16ms | Tot: 1s217ms | Val Loss: 0.618 | Val Acc: 79.652% (5257/6600 66/100 ===============================>...]  Step: 14ms | Tot: 1s669ms | Val Loss: 0.615 | Val Acc: 79.793% (7341/9200 92/100 \n\nEpoch: 11\n [==================================>]  Step: 34ms | Tot: 19s873ms | Train Loss: 0.450 | Train Acc: 84.436% (42218/50000 391/391 \n [==================================>]  Step: 14ms | Tot: 1s978ms | Val Loss: 0.630 | Val Acc: 78.810% (7881/10000 100/100 \n\nEpoch: 12\n [==================================>]  Step: 33ms | Tot: 19s835ms | Train Loss: 0.441 | Train Acc: 84.928% (42464/50000 391/391 ...............................]  Step: 50ms | Tot: 2s27ms | Train Loss: 0.441 | Train Acc: 85.271% (4475/5248 41/391 \n [==================================>]  Step: 14ms | Tot: 1s758ms | Val Loss: 0.482 | Val Acc: 84.070% (8407/10000 100/100 ====>..........................]  Step: 15ms | Tot: 419ms | Val Loss: 0.508 | Val Acc: 82.583% (1982/2400 24/100 \nSaving..\n\nEpoch: 13\n [==================================>]  Step: 34ms | Tot: 19s790ms | Train Loss: 0.429 | Train Acc: 85.246% (42623/50000 391/391 =========>........................]  Step: 50ms | Tot: 6s189ms | Train Loss: 0.423 | Train Acc: 85.442% (13452/15744 123/391 =====================>.............]  Step: 50ms | Tot: 11s931ms | Train Loss: 0.429 | Train Acc: 85.338% (25779/30208 236/391 \n [==================================>]  Step: 13ms | Tot: 1s806ms | Val Loss: 0.511 | Val Acc: 82.630% (8263/10000 100/100 \n\nEpoch: 14\n [==================================>]  Step: 34ms | Tot: 19s796ms | Train Loss: 0.423 | Train Acc: 85.474% (42737/50000 391/391 ====================>..............]  Step: 51ms | Tot: 11s601ms | Train Loss: 0.418 | Train Acc: 85.528% (25070/29312 229/391 \n [==================================>]  Step: 13ms | Tot: 1s841ms | Val Loss: 0.537 | Val Acc: 82.000% (8200/10000 100/100 \n\nEpoch: 15\n [==================================>]  Step: 34ms | Tot: 19s802ms | Train Loss: 0.413 | Train Acc: 85.718% (42859/50000 391/391 ==================================>]  Step: 51ms | Tot: 19s312ms | Train Loss: 0.412 | Train Acc: 85.749% (41818/48768 381/391 \n [==================================>]  Step: 15ms | Tot: 1s895ms | Val Loss: 0.566 | Val Acc: 80.500% (8050/10000 100/100 ========================>........]  Step: 23ms | Tot: 1s493ms | Val Loss: 0.569 | Val Acc: 80.615% (6288/7800 78/100 \n\nEpoch: 16\n [==================================>]  Step: 33ms | Tot: 19s813ms | Train Loss: 0.404 | Train Acc: 86.356% (43178/50000 391/391 \n [==================================>]  Step: 13ms | Tot: 1s991ms | Val Loss: 0.505 | Val Acc: 83.190% (8319/10000 100/100 ===========================>.....]  Step: 15ms | Tot: 1s712ms | Val Loss: 0.510 | Val Acc: 83.116% (7148/8600 86/100 \n\nEpoch: 17\n [==================================>]  Step: 34ms | Tot: 19s767ms | Train Loss: 0.399 | Train Acc: 86.312% (43156/50000 391/391 ...............................]  Step: 50ms | Tot: 1s628ms | Train Loss: 0.392 | Train Acc: 86.316% (3646/4224 33/391 =====================>.............]  Step: 50ms | Tot: 12s17ms | Train Loss: 0.400 | Train Acc: 86.217% (26265/30464 238/391 ======================>............]  Step: 50ms | Tot: 12s825ms | Train Loss: 0.401 | Train Acc: 86.153% (28010/32512 254/391 =======================>...........]  Step: 50ms | Tot: 13s481ms | Train Loss: 0.402 | Train Acc: 86.104% (29427/34176 267/391 =======================>...........]  Step: 50ms | Tot: 13s582ms | Train Loss: 0.402 | Train Acc: 86.120% (29653/34432 269/391 ========================>..........]  Step: 50ms | Tot: 13s734ms | Train Loss: 0.402 | Train Acc: 86.139% (29990/34816 272/391 \n [==================================>]  Step: 19ms | Tot: 1s877ms | Val Loss: 0.536 | Val Acc: 82.660% (8266/10000 100/100 \n\nEpoch: 18\n [==================================>]  Step: 33ms | Tot: 19s772ms | Train Loss: 0.397 | Train Acc: 86.554% (43277/50000 391/391 ==================>................]  Step: 50ms | Tot: 10s535ms | Train Loss: 0.392 | Train Acc: 86.760% (23099/26624 208/391 \n [==================================>]  Step: 13ms | Tot: 1s878ms | Val Loss: 0.516 | Val Acc: 81.830% (8183/10000 100/100 \n\nEpoch: 19\n [==================================>]  Step: 33ms | Tot: 19s777ms | Train Loss: 0.387 | Train Acc: 86.890% (43445/50000 391/391 =>.............................]  Step: 49ms | Tot: 3s348ms | Train Loss: 0.353 | Train Acc: 88.130% (7558/8576 67/391 ================================>..]  Step: 52ms | Tot: 18s175ms | Train Loss: 0.386 | Train Acc: 86.955% (40069/46080 360/391 \n [==================================>]  Step: 14ms | Tot: 1s776ms | Val Loss: 0.493 | Val Acc: 83.930% (8393/10000 100/100 ===============>...............]  Step: 14ms | Tot: 967ms | Val Loss: 0.495 | Val Acc: 84.018% (4705/5600 56/100 \n\nEpoch: 20\n [==================================>]  Step: 34ms | Tot: 19s791ms | Train Loss: 0.383 | Train Acc: 86.812% (43406/50000 391/391 ...............................]  Step: 50ms | Tot: 1s715ms | Train Loss: 0.358 | Train Acc: 87.656% (3927/4480 35/391 ====>..............................]  Step: 51ms | Tot: 2s375ms | Train Loss: 0.363 | Train Acc: 87.402% (5370/6144 48/391 =================>.................]  Step: 51ms | Tot: 9s790ms | Train Loss: 0.385 | Train Acc: 86.755% (21543/24832 194/391 ========================>..........]  Step: 51ms | Tot: 13s812ms | Train Loss: 0.383 | Train Acc: 86.848% (30348/34944 273/391 \n [==================================>]  Step: 15ms | Tot: 1s964ms | Val Loss: 0.445 | Val Acc: 84.930% (8493/10000 100/100 \nSaving..\n\nEpoch: 21\n [==================================>]  Step: 34ms | Tot: 19s936ms | Train Loss: 0.376 | Train Acc: 87.280% (43640/50000 391/391 .............................]  Step: 50ms | Tot: 833ms | Train Loss: 0.319 | Train Acc: 89.154% (1940/2176 17/391 ====>..............................]  Step: 50ms | Tot: 2s300ms | Train Loss: 0.347 | Train Acc: 88.179% (5192/5888 46/391 ====>..............................]  Step: 50ms | Tot: 2s350ms | Train Loss: 0.346 | Train Acc: 88.265% (5310/6016 47/391 ==============>....................]  Step: 50ms | Tot: 8s534ms | Train Loss: 0.373 | Train Acc: 87.249% (18762/21504 168/391 ==================>................]  Step: 51ms | Tot: 10s418ms | Train Loss: 0.374 | Train Acc: 87.317% (22912/26240 205/391 ===================>...............]  Step: 51ms | Tot: 10s984ms | Train Loss: 0.372 | Train Acc: 87.428% (24172/27648 216/391 ==========================>........]  Step: 51ms | Tot: 15s3ms | Train Loss: 0.375 | Train Acc: 87.365% (32989/37760 295/391 \n [==================================>]  Step: 13ms | Tot: 1s935ms | Val Loss: 0.533 | Val Acc: 82.410% (8241/10000 100/100 \n\nEpoch: 22\n [==================================>]  Step: 34ms | Tot: 19s860ms | Train Loss: 0.372 | Train Acc: 87.392% (43696/50000 391/391 ==============================>....]  Step: 50ms | Tot: 17s621ms | Train Loss: 0.371 | Train Acc: 87.448% (38841/44416 347/391 \n [==================================>]  Step: 13ms | Tot: 1s890ms | Val Loss: 0.555 | Val Acc: 82.140% (8214/10000 100/100 \n\nEpoch: 23\n [==================================>]  Step: 34ms | Tot: 19s727ms | Train Loss: 0.363 | Train Acc: 87.574% (43787/50000 391/391 ...............................]  Step: 51ms | Tot: 1s167ms | Train Loss: 0.348 | Train Acc: 88.411% (2716/3072 24/391 ====>..............................]  Step: 50ms | Tot: 2s435ms | Train Loss: 0.360 | Train Acc: 87.755% (5504/6272 49/391 =========>.........................]  Step: 50ms | Tot: 5s459ms | Train Loss: 0.360 | Train Acc: 87.686% (12234/13952 109/391 ==============>....................]  Step: 51ms | Tot: 7s964ms | Train Loss: 0.362 | Train Acc: 87.584% (17713/20224 158/391 ==================>................]  Step: 51ms | Tot: 10s539ms | Train Loss: 0.367 | Train Acc: 87.283% (23350/26752 209/391 \n [==================================>]  Step: 13ms | Tot: 1s920ms | Val Loss: 0.533 | Val Acc: 82.140% (8214/10000 100/100 \n\nEpoch: 24\n [==================================>]  Step: 34ms | Tot: 19s842ms | Train Loss: 0.359 | Train Acc: 87.696% (43848/50000 391/391 ...............................]  Step: 51ms | Tot: 2s35ms | Train Loss: 0.344 | Train Acc: 88.605% (4650/5248 41/391 \n [==================================>]  Step: 13ms | Tot: 1s829ms | Val Loss: 0.566 | Val Acc: 81.260% (8126/10000 100/100 ...........................]  Step: 18ms | Tot: 73ms | Val Loss: 0.509 | Val Acc: 82.200% (411/500 5/100 \n\nEpoch: 25\n [==================================>]  Step: 33ms | Tot: 19s966ms | Train Loss: 0.357 | Train Acc: 87.692% (43846/50000 391/391 \n [==================================>]  Step: 13ms | Tot: 1s873ms | Val Loss: 0.453 | Val Acc: 84.850% (8485/10000 100/100 \n\nEpoch: 26\n [==================================>]  Step: 33ms | Tot: 19s744ms | Train Loss: 0.355 | Train Acc: 88.024% (44012/50000 391/391 =======================>...........]  Step: 50ms | Tot: 13s354ms | Train Loss: 0.352 | Train Acc: 88.192% (29802/33792 264/391 ==========================>........]  Step: 51ms | Tot: 15s216ms | Train Loss: 0.352 | Train Acc: 88.198% (33981/38528 301/391 =============================>.....]  Step: 51ms | Tot: 16s682ms | Train Loss: 0.354 | Train Acc: 88.094% (37211/42240 330/391 \n [==================================>]  Step: 13ms | Tot: 1s794ms | Val Loss: 0.482 | Val Acc: 83.710% (8371/10000 100/100 ====>..........................]  Step: 15ms | Tot: 499ms | Val Loss: 0.506 | Val Acc: 83.346% (2167/2600 26/100 ===================>...............]  Step: 19ms | Tot: 1s18ms | Val Loss: 0.486 | Val Acc: 83.768% (4691/5600 56/100 \n\nEpoch: 27\n [==================================>]  Step: 33ms | Tot: 19s810ms | Train Loss: 0.352 | Train Acc: 88.006% (44003/50000 391/391 ================================>..]  Step: 51ms | Tot: 18s201ms | Train Loss: 0.353 | Train Acc: 87.992% (40434/45952 359/391 \n [==================================>]  Step: 13ms | Tot: 1s760ms | Val Loss: 0.429 | Val Acc: 85.910% (8591/10000 100/100 ===============>...............]  Step: 18ms | Tot: 1s7ms | Val Loss: 0.440 | Val Acc: 85.667% (4883/5700 57/100 \nSaving..\n\nEpoch: 28\n [==================================>]  Step: 33ms | Tot: 19s779ms | Train Loss: 0.346 | Train Acc: 88.270% (44135/50000 391/391 ...............................]  Step: 51ms | Tot: 1s876ms | Train Loss: 0.315 | Train Acc: 89.227% (4340/4864 38/391 ================>..................]  Step: 50ms | Tot: 9s77ms | Train Loss: 0.337 | Train Acc: 88.633% (20421/23040 180/391 \n [==================================>]  Step: 14ms | Tot: 1s872ms | Val Loss: 0.531 | Val Acc: 83.060% (8306/10000 100/100 ============================>....]  Step: 19ms | Tot: 1s668ms | Val Loss: 0.538 | Val Acc: 82.955% (7383/8900 89/100 \n\nEpoch: 29\n [==================================>]  Step: 35ms | Tot: 19s715ms | Train Loss: 0.345 | Train Acc: 88.170% (44085/50000 391/391 ======================>............]  Step: 51ms | Tot: 12s626ms | Train Loss: 0.341 | Train Acc: 88.210% (28340/32128 251/391 =============================>.....]  Step: 50ms | Tot: 16s882ms | Train Loss: 0.343 | Train Acc: 88.193% (37817/42880 335/391 \n [==================================>]  Step: 13ms | Tot: 1s847ms | Val Loss: 0.608 | Val Acc: 80.900% (8090/10000 100/100 \n\nEpoch: 30\n [==================================>]  Step: 33ms | Tot: 19s691ms | Train Loss: 0.341 | Train Acc: 88.402% (44201/50000 391/391 ==========>.......................]  Step: 51ms | Tot: 6s475ms | Train Loss: 0.330 | Train Acc: 88.705% (14647/16512 129/391 ===============>...................]  Step: 51ms | Tot: 8s839ms | Train Loss: 0.332 | Train Acc: 88.712% (19985/22528 176/391 =====================>.............]  Step: 51ms | Tot: 11s865ms | Train Loss: 0.335 | Train Acc: 88.602% (26765/30208 236/391 =====================>.............]  Step: 50ms | Tot: 12s16ms | Train Loss: 0.334 | Train Acc: 88.621% (27111/30592 239/391 \n [==================================>]  Step: 14ms | Tot: 1s861ms | Val Loss: 0.438 | Val Acc: 85.470% (8547/10000 100/100 \n\nEpoch: 31\n [==================================>]  Step: 33ms | Tot: 19s802ms | Train Loss: 0.351 | Train Acc: 88.154% (44077/50000 391/391 ======>..........................]  Step: 51ms | Tot: 5s89ms | Train Loss: 0.344 | Train Acc: 88.374% (11425/12928 101/391 \n [==================================>]  Step: 13ms | Tot: 1s819ms | Val Loss: 0.574 | Val Acc: 81.850% (8185/10000 100/100 ========================>........]  Step: 17ms | Tot: 1s406ms | Val Loss: 0.576 | Val Acc: 81.947% (6228/7600 76/100 \n\nEpoch: 32\n [==================================>]  Step: 33ms | Tot: 19s698ms | Train Loss: 0.341 | Train Acc: 88.566% (44283/50000 391/391 \n [==================================>]  Step: 14ms | Tot: 1s773ms | Val Loss: 0.527 | Val Acc: 82.960% (8296/10000 100/100 ==============>................]  Step: 17ms | Tot: 923ms | Val Loss: 0.533 | Val Acc: 82.736% (4385/5300 53/100 \n\nEpoch: 33\n [==================================>]  Step: 33ms | Tot: 19s742ms | Train Loss: 0.338 | Train Acc: 88.478% (44239/50000 391/391 =>.............................]  Step: 50ms | Tot: 2s883ms | Train Loss: 0.322 | Train Acc: 88.834% (6595/7424 58/391 ============================>......]  Step: 51ms | Tot: 15s948ms | Train Loss: 0.335 | Train Acc: 88.454% (35778/40448 316/391 \n [==================================>]  Step: 14ms | Tot: 1s862ms | Val Loss: 0.402 | Val Acc: 86.700% (8670/10000 100/100 \nSaving..\n\nEpoch: 34\n [==================================>]  Step: 34ms | Tot: 19s768ms | Train Loss: 0.337 | Train Acc: 88.494% (44247/50000 391/391 ===============>..................]  Step: 51ms | Tot: 9s278ms | Train Loss: 0.324 | Train Acc: 89.003% (20848/23424 183/391 =======================>...........]  Step: 50ms | Tot: 13s204ms | Train Loss: 0.331 | Train Acc: 88.667% (29622/33408 261/391 \n [==================================>]  Step: 17ms | Tot: 1s781ms | Val Loss: 0.549 | Val Acc: 82.440% (8244/10000 100/100 ===========>...................]  Step: 15ms | Tot: 783ms | Val Loss: 0.538 | Val Acc: 82.667% (3720/4500 45/100 ================>..................]  Step: 15ms | Tot: 853ms | Val Loss: 0.538 | Val Acc: 82.592% (4047/4900 49/100 ======================>............]  Step: 18ms | Tot: 1s140ms | Val Loss: 0.548 | Val Acc: 82.364% (5436/6600 66/100 \n\nEpoch: 35\n [==================================>]  Step: 33ms | Tot: 19s739ms | Train Loss: 0.330 | Train Acc: 88.804% (44402/50000 391/391 ===================>...............]  Step: 51ms | Tot: 11s25ms | Train Loss: 0.316 | Train Acc: 89.380% (25055/28032 219/391 \n [==================================>]  Step: 14ms | Tot: 1s782ms | Val Loss: 0.455 | Val Acc: 85.070% (8507/10000 100/100 \n\nEpoch: 36\n [==================================>]  Step: 33ms | Tot: 19s731ms | Train Loss: 0.329 | Train Acc: 88.782% (44391/50000 391/391 ..........................]  Step: 50ms | Tot: 359ms | Train Loss: 0.283 | Train Acc: 89.844% (920/1024 8/391 ================>..................]  Step: 51ms | Tot: 9s478ms | Train Loss: 0.320 | Train Acc: 88.959% (21407/24064 188/391 ==============================>....]  Step: 51ms | Tot: 17s171ms | Train Loss: 0.328 | Train Acc: 88.778% (38636/43520 340/391 ==============================>....]  Step: 50ms | Tot: 17s523ms | Train Loss: 0.328 | Train Acc: 88.788% (39436/44416 347/391 \n [==================================>]  Step: 14ms | Tot: 1s924ms | Val Loss: 0.481 | Val Acc: 84.640% (8464/10000 100/100 ==========================>......]  Step: 20ms | Tot: 1s600ms | Val Loss: 0.483 | Val Acc: 84.543% (6848/8100 81/100 \n\nEpoch: 37\n [==================================>]  Step: 33ms | Tot: 19s781ms | Train Loss: 0.330 | Train Acc: 88.664% (44332/50000 391/391 ==============================>....]  Step: 50ms | Tot: 17s417ms | Train Loss: 0.330 | Train Acc: 88.647% (39033/44032 344/391 \n [==================================>]  Step: 14ms | Tot: 1s767ms | Val Loss: 0.682 | Val Acc: 78.470% (7847/10000 100/100 \n\nEpoch: 38\n [==================================>]  Step: 33ms | Tot: 19s801ms | Train Loss: 0.322 | Train Acc: 88.874% (44437/50000 391/391 ============>.....................]  Step: 50ms | Tot: 7s915ms | Train Loss: 0.324 | Train Acc: 88.953% (17876/20096 157/391 ===============================>...]  Step: 51ms | Tot: 17s682ms | Train Loss: 0.320 | Train Acc: 88.966% (39743/44672 349/391 \n [==================================>]  Step: 17ms | Tot: 1s814ms | Val Loss: 0.493 | Val Acc: 84.140% (8414/10000 100/100 ===========>...................]  Step: 16ms | Tot: 794ms | Val Loss: 0.501 | Val Acc: 83.933% (3777/4500 45/100 \n\nEpoch: 39\n [==================================>]  Step: 34ms | Tot: 19s786ms | Train Loss: 0.327 | Train Acc: 88.934% (44467/50000 391/391 ...............................]  Step: 51ms | Tot: 1s475ms | Train Loss: 0.288 | Train Acc: 90.339% (3469/3840 30/391 =======>...........................]  Step: 50ms | Tot: 4s254ms | Train Loss: 0.303 | Train Acc: 89.632% (9752/10880 85/391 ===========>.......................]  Step: 50ms | Tot: 6s547ms | Train Loss: 0.314 | Train Acc: 89.261% (14853/16640 130/391 =================================>.]  Step: 51ms | Tot: 19s244ms | Train Loss: 0.329 | Train Acc: 88.859% (43221/48640 380/391 \n [==================================>]  Step: 13ms | Tot: 1s788ms | Val Loss: 0.525 | Val Acc: 83.470% (8347/10000 100/100 ============>..................]  Step: 15ms | Tot: 872ms | Val Loss: 0.532 | Val Acc: 83.020% (4068/4900 49/100 ===========================>.......]  Step: 15ms | Tot: 1s426ms | Val Loss: 0.527 | Val Acc: 83.438% (6675/8000 80/100 \n\nEpoch: 40\n [==================================>]  Step: 34ms | Tot: 19s760ms | Train Loss: 0.326 | Train Acc: 88.858% (44429/50000 391/391 =============================>.....]  Step: 50ms | Tot: 16s740ms | Train Loss: 0.324 | Train Acc: 88.982% (37700/42368 331/391 \n [==================================>]  Step: 13ms | Tot: 1s894ms | Val Loss: 0.422 | Val Acc: 85.940% (8594/10000 100/100 \n\nEpoch: 41\n [==================================>]  Step: 33ms | Tot: 19s708ms | Train Loss: 0.325 | Train Acc: 88.884% (44442/50000 391/391 ............................]  Step: 51ms | Tot: 459ms | Train Loss: 0.310 | Train Acc: 88.594% (1134/1280 10/391 =============================>.....]  Step: 50ms | Tot: 16s396ms | Train Loss: 0.327 | Train Acc: 88.707% (36902/41600 325/391 ===============================>...]  Step: 49ms | Tot: 18s6ms | Train Loss: 0.327 | Train Acc: 88.796% (40576/45696 357/391 \n [==================================>]  Step: 25ms | Tot: 1s850ms | Val Loss: 0.476 | Val Acc: 84.810% (8481/10000 100/100 \n\nEpoch: 42\n [==================================>]  Step: 34ms | Tot: 19s727ms | Train Loss: 0.317 | Train Acc: 89.278% (44639/50000 391/391 \n [==================================>]  Step: 14ms | Tot: 1s810ms | Val Loss: 0.663 | Val Acc: 80.060% (8006/10000 100/100 ===========>...................]  Step: 16ms | Tot: 855ms | Val Loss: 0.677 | Val Acc: 79.804% (3671/4600 46/100 \n\nEpoch: 43\n [==================================>]  Step: 33ms | Tot: 19s697ms | Train Loss: 0.306 | Train Acc: 89.576% (44788/50000 391/391 .............................]  Step: 50ms | Tot: 765ms | Train Loss: 0.298 | Train Acc: 89.941% (1842/2048 16/391 ===>...............................]  Step: 50ms | Tot: 2s21ms | Train Loss: 0.296 | Train Acc: 89.825% (4714/5248 41/391 =======>...........................]  Step: 50ms | Tot: 4s338ms | Train Loss: 0.293 | Train Acc: 90.005% (10023/11136 87/391 =========>.........................]  Step: 50ms | Tot: 5s343ms | Train Loss: 0.290 | Train Acc: 90.129% (12344/13696 107/391 =========>.........................]  Step: 50ms | Tot: 5s594ms | Train Loss: 0.289 | Train Acc: 90.165% (12926/14336 112/391 ================>..................]  Step: 50ms | Tot: 9s191ms | Train Loss: 0.290 | Train Acc: 90.168% (21121/23424 183/391 ================>..................]  Step: 50ms | Tot: 9s444ms | Train Loss: 0.291 | Train Acc: 90.122% (21687/24064 188/391 \n [==================================>]  Step: 13ms | Tot: 1s836ms | Val Loss: 0.627 | Val Acc: 80.090% (8009/10000 100/100 ===============================>.]  Step: 15ms | Tot: 1s771ms | Val Loss: 0.630 | Val Acc: 80.062% (7686/9600 96/100 \n\nEpoch: 44\n [==================================>]  Step: 33ms | Tot: 19s749ms | Train Loss: 0.315 | Train Acc: 89.332% (44666/50000 391/391 >..............................]  Step: 51ms | Tot: 2s477ms | Train Loss: 0.307 | Train Acc: 89.297% (5715/6400 50/391 =======================>...........]  Step: 51ms | Tot: 13s240ms | Train Loss: 0.315 | Train Acc: 89.417% (29987/33536 262/391 \n [==================================>]  Step: 14ms | Tot: 1s883ms | Val Loss: 0.702 | Val Acc: 78.230% (7823/10000 100/100 \n\nEpoch: 45\n [==================================>]  Step: 33ms | Tot: 19s836ms | Train Loss: 0.311 | Train Acc: 89.464% (44732/50000 391/391 ...............................]  Step: 50ms | Tot: 1s569ms | Train Loss: 0.280 | Train Acc: 90.405% (3703/4096 32/391 ===============>...................]  Step: 51ms | Tot: 8s935ms | Train Loss: 0.302 | Train Acc: 89.641% (20309/22656 177/391 \n [==================================>]  Step: 14ms | Tot: 1s769ms | Val Loss: 0.422 | Val Acc: 85.760% (8576/10000 100/100 =======>.......................]  Step: 18ms | Tot: 581ms | Val Loss: 0.433 | Val Acc: 85.057% (2977/3500 35/100 \n\nEpoch: 46\n [==================================>]  Step: 33ms | Tot: 19s737ms | Train Loss: 0.310 | Train Acc: 89.384% (44692/50000 391/391 ==============>...................]  Step: 50ms | Tot: 8s898ms | Train Loss: 0.297 | Train Acc: 89.981% (20386/22656 177/391 ==================>................]  Step: 50ms | Tot: 10s358ms | Train Loss: 0.300 | Train Acc: 89.882% (23700/26368 206/391 =============================>.....]  Step: 50ms | Tot: 16s764ms | Train Loss: 0.307 | Train Acc: 89.557% (38058/42496 332/391 =============================>.....]  Step: 50ms | Tot: 16s865ms | Train Loss: 0.306 | Train Acc: 89.575% (38295/42752 334/391 \n [==================================>]  Step: 15ms | Tot: 1s986ms | Val Loss: 0.513 | Val Acc: 83.470% (8347/10000 100/100 \n\nEpoch: 47\n [==================================>]  Step: 33ms | Tot: 19s810ms | Train Loss: 0.306 | Train Acc: 89.618% (44809/50000 391/391 =>.............................]  Step: 51ms | Tot: 2s893ms | Train Loss: 0.289 | Train Acc: 90.140% (6692/7424 58/391 ======>............................]  Step: 51ms | Tot: 3s855ms | Train Loss: 0.284 | Train Acc: 90.442% (8914/9856 77/391 \n [==================================>]  Step: 13ms | Tot: 1s851ms | Val Loss: 0.387 | Val Acc: 87.070% (8707/10000 100/100 \nSaving..\n\nEpoch: 48\n [==================================>]  Step: 34ms | Tot: 19s767ms | Train Loss: 0.305 | Train Acc: 89.618% (44809/50000 391/391 ..............................]  Step: 50ms | Tot: 1s114ms | Train Loss: 0.278 | Train Acc: 90.353% (2660/2944 23/391 =============================>.....]  Step: 50ms | Tot: 16s445ms | Train Loss: 0.305 | Train Acc: 89.575% (37263/41600 325/391 \n [==================================>]  Step: 14ms | Tot: 1s898ms | Val Loss: 0.601 | Val Acc: 80.680% (8068/10000 100/100 =======>.......................]  Step: 19ms | Tot: 680ms | Val Loss: 0.590 | Val Acc: 81.029% (2755/3400 34/100 \n\nEpoch: 49\n [==================================>]  Step: 33ms | Tot: 19s753ms | Train Loss: 0.305 | Train Acc: 89.638% (44819/50000 391/391 =================================>.]  Step: 52ms | Tot: 18s756ms | Train Loss: 0.305 | Train Acc: 89.629% (42563/47488 371/391 \n [==================================>]  Step: 14ms | Tot: 1s763ms | Val Loss: 0.418 | Val Acc: 86.010% (8601/10000 100/100 \n\nEpoch: 50\n [==================================>]  Step: 33ms | Tot: 19s833ms | Train Loss: 0.301 | Train Acc: 89.754% (44877/50000 391/391 ===================>...............]  Step: 51ms | Tot: 11s136ms | Train Loss: 0.294 | Train Acc: 89.829% (25181/28032 219/391 =========================>.........]  Step: 50ms | Tot: 14s684ms | Train Loss: 0.298 | Train Acc: 89.795% (33217/36992 289/391 ===========================>.......]  Step: 51ms | Tot: 15s440ms | Train Loss: 0.298 | Train Acc: 89.769% (34931/38912 304/391 ==============================>....]  Step: 50ms | Tot: 17s160ms | Train Loss: 0.299 | Train Acc: 89.786% (38845/43264 338/391 \n [==================================>]  Step: 14ms | Tot: 1s832ms | Val Loss: 0.457 | Val Acc: 85.070% (8507/10000 100/100 \n\nEpoch: 51\n [==================================>]  Step: 34ms | Tot: 19s889ms | Train Loss: 0.295 | Train Acc: 89.912% (44956/50000 391/391 ========================>..........]  Step: 51ms | Tot: 14s70ms | Train Loss: 0.288 | Train Acc: 90.090% (31827/35328 276/391 \n [==================================>]  Step: 14ms | Tot: 1s829ms | Val Loss: 0.436 | Val Acc: 85.200% (8520/10000 100/100 \n\nEpoch: 52\n [==================================>]  Step: 33ms | Tot: 19s781ms | Train Loss: 0.301 | Train Acc: 89.694% (44847/50000 391/391 =========>........................]  Step: 51ms | Tot: 6s98ms | Train Loss: 0.295 | Train Acc: 89.857% (13917/15488 121/391 \n [==================================>]  Step: 14ms | Tot: 1s971ms | Val Loss: 0.513 | Val Acc: 83.500% (8350/10000 100/100 ========================>........]  Step: 31ms | Tot: 1s539ms | Val Loss: 0.513 | Val Acc: 83.577% (6519/7800 78/100 \n\nEpoch: 53\n [==================================>]  Step: 34ms | Tot: 19s794ms | Train Loss: 0.296 | Train Acc: 89.970% (44985/50000 391/391 ===============>..................]  Step: 51ms | Tot: 9s461ms | Train Loss: 0.292 | Train Acc: 90.090% (21564/23936 187/391 ======================>............]  Step: 50ms | Tot: 12s501ms | Train Loss: 0.292 | Train Acc: 90.078% (28479/31616 247/391 ======================>............]  Step: 51ms | Tot: 12s703ms | Train Loss: 0.292 | Train Acc: 90.062% (28935/32128 251/391 \n [==================================>]  Step: 19ms | Tot: 1s903ms | Val Loss: 0.375 | Val Acc: 87.740% (8774/10000 100/100 ===>...........................]  Step: 15ms | Tot: 407ms | Val Loss: 0.391 | Val Acc: 87.739% (2018/2300 23/100 \nSaving..\n\nEpoch: 54\n [============>......................]  Step: 49ms | Tot: 6s904ms | Train Loss: 0.276 | Train Acc: 90.750% (15914/17536 137/391 ==========>........................]  Step: 50ms | Tot: 5s931ms | Train Loss: 0.279 | Train Acc: 90.665% (13694/15104 118/391 \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test Predictions Generation","metadata":{}},{"cell_type":"code","source":"# Define a custom dataset class for the test dataset\nclass CIFAR10Test(Dataset):\n    def __init__(self, file_path, transform=None):\n        # Unpickle the test dataset file\n        test_data_dict = unpickle(file_path)\n        self.data = test_data_dict[b'data']\n        self.ids = test_data_dict[b'ids']\n        self.data = self.data.reshape(-1, 3, 32, 32).transpose((0, 2, 3, 1))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        img = self.data[index]\n        img = Image.fromarray(img)\n        if self.transform:\n            img = self.transform(img)\n        return img, self.ids[index]\n    \ntest_file_path= '/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar_test_nolabels.pkl'\ntest_dataset = CIFAR10Test(file_path=test_file_path, transform=transform_validation)\ntestloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndef generate_predictions():\n    net.eval()  # Ensure the model is in evaluation mode\n    predictions = []\n\n    with torch.no_grad():\n        for inputs, ids in testloader:\n            inputs = inputs.to(device)\n            outputs = net(inputs)\n            _, predicted = outputs.max(1)\n            \n            # Collect the IDs and their corresponding predicted labels\n            for id, label in zip(ids, predicted):\n                predictions.append([id.item(), label.item()])\n\n    # Convert predictions to a DataFrame and save as CSV\n    df = pd.DataFrame(predictions, columns=['ID', 'Labels'])\n    df.to_csv('predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T19:55:32.912119Z","iopub.execute_input":"2024-04-11T19:55:32.912510Z","iopub.status.idle":"2024-04-11T19:55:36.423834Z","shell.execute_reply.started":"2024-04-11T19:55:32.912478Z","shell.execute_reply":"2024-04-11T19:55:36.422937Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"generate_predictions()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T19:55:32.912119Z","iopub.execute_input":"2024-04-11T19:55:32.912510Z","iopub.status.idle":"2024-04-11T19:55:36.423834Z","shell.execute_reply.started":"2024-04-11T19:55:32.912478Z","shell.execute_reply":"2024-04-11T19:55:36.422937Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the Training ","metadata":{}},{"cell_type":"code","source":"# Plotting training and validation loss\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\nplt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Plotting training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\nplt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}